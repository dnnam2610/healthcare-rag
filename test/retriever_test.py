import google.generativeai as genai
import json
from typing import List, Dict
import pandas as pd
from datetime import datetime
import os

# Cáº¥u hÃ¬nh API key
genai.configure(api_key="AIzaSyAYOLcSrYq1CNJHNo42u0x6Decf7g3QB_s")

class RAGMetrics:
    def __init__(self, model_name="gemini-2.5-flash"):
        self.model = genai.GenerativeModel(model_name)
    
    def calculate_context_precision(
        self, 
        question: str, 
        contexts: List[str], 
        ground_truth: str
    ) -> float:
        """
        Context Precision: Äo lÆ°á»ng Ä‘á»™ chÃ­nh xÃ¡c cá»§a cÃ¡c context Ä‘Æ°á»£c retrieve.
        TÃ­nh tá»· lá»‡ cÃ¡c context cÃ³ liÃªn quan trong tá»•ng sá»‘ context Ä‘Æ°á»£c retrieve.
        
        Args:
            question: CÃ¢u há»i cá»§a user
            contexts: Danh sÃ¡ch cÃ¡c context Ä‘Æ°á»£c retrieve (theo thá»© tá»± rank)
            ground_truth: CÃ¢u tráº£ lá»i Ä‘Ãºng/ground truth
        
        Returns:
            Context precision score (0-1)
        """
        relevance_scores = []
        
        for i, context in enumerate(contexts):
            prompt = f"""
ÄÃ¡nh giÃ¡ xem context sau cÃ³ liÃªn quan vÃ  há»¯u Ã­ch Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i khÃ´ng.

CÃ¢u há»i: {question}
Ground Truth: {ground_truth}
Context: {context}

Tráº£ lá»i chá»‰ báº±ng JSON vá»›i format:
{{"relevant": true/false, "reason": "lÃ½ do ngáº¯n gá»n"}}
"""
            
            try:
                response = self.model.generate_content(prompt)
                result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
                relevance_scores.append(1 if result['relevant'] else 0)
            except Exception as e:
                print(f"Error processing context {i}: {e}")
                relevance_scores.append(0)
        
        if not relevance_scores:
            return 0.0
        
        # Context Precision = (sá»‘ context relevant) / (tá»•ng sá»‘ context)
        precision = sum(relevance_scores) / len(relevance_scores)
        return precision
    
    def calculate_context_recall(
        self, 
        ground_truth: str, 
        contexts: List[str]
    ) -> float:
        """
        Context Recall: Äo lÆ°á»ng kháº£ nÄƒng retrieve Ä‘Æ°á»£c táº¥t cáº£ thÃ´ng tin cáº§n thiáº¿t.
        TÃ­nh tá»· lá»‡ cÃ¡c thÃ´ng tin trong ground truth cÃ³ xuáº¥t hiá»‡n trong contexts.
        
        Args:
            ground_truth: CÃ¢u tráº£ lá»i Ä‘Ãºng/ground truth
            contexts: Danh sÃ¡ch cÃ¡c context Ä‘Æ°á»£c retrieve
        
        Returns:
            Context recall score (0-1)
        """
        combined_contexts = "\n\n".join(contexts)
        
        prompt = f"""
PhÃ¢n tÃ­ch xem cÃ¡c context Ä‘Ã£ retrieve cÃ³ chá»©a Ä‘á»§ thÃ´ng tin Ä‘á»ƒ táº¡o ra ground truth khÃ´ng.

Ground Truth: {ground_truth}

Contexts Ä‘Ã£ retrieve:
{combined_contexts}

HÃ£y:
1. Chia ground truth thÃ nh cÃ¡c statement/thÃ´ng tin riÃªng biá»‡t
2. ÄÃ¡nh giÃ¡ xem má»—i statement cÃ³ Ä‘Æ°á»£c há»— trá»£ bá»Ÿi contexts khÃ´ng

Tráº£ lá»i báº±ng JSON vá»›i format:
{{
    "statements": [
        {{"statement": "thÃ´ng tin 1", "supported": true/false}},
        {{"statement": "thÃ´ng tin 2", "supported": true/false}}
    ],
    "recall": 0.0-1.0
}}
"""
        
        try:
            response = self.model.generate_content(prompt)
            result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
            return result['recall']
        except Exception as e:
            print(f"Error calculating recall: {e}")
            
            # Fallback: tÃ­nh báº±ng keyword matching Ä‘Æ¡n giáº£n
            gt_words = set(ground_truth.lower().split())
            context_words = set(combined_contexts.lower().split())
            overlap = len(gt_words.intersection(context_words))
            return overlap / len(gt_words) if gt_words else 0.0


class MetricsLogger:
    """Class Ä‘á»ƒ lÆ°u trá»¯ vÃ  quáº£n lÃ½ káº¿t quáº£ test"""
    
    def __init__(self, csv_path='rag_metrics_results.csv'):
        self.csv_path = csv_path
        self.results = []
        
    def add_result(
        self, 
        sample_id: str,
        question: str,
        ground_truth: str,
        precision: float,
        recall: float,
        num_contexts: int,
        retrieved_doc_ids: List[str] = None,
        retriever_type: str = None,
        metadata: Dict = None
    ):
        """ThÃªm má»™t káº¿t quáº£ test"""
        result = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'sample_id': sample_id,
            'question': question[:100],  # Giá»›i háº¡n Ä‘á»™ dÃ i
            'ground_truth': ground_truth[:100],
            'precision': precision,
            'recall': recall,
            'num_contexts': num_contexts,
            'retrieved_doc_ids': ','.join(map(str, retrieved_doc_ids)) if retrieved_doc_ids else '',
            'retriever_type': retriever_type or 'unknown'
        }
        
        # ThÃªm metadata náº¿u cÃ³
        if metadata:
            result.update(metadata)
            
        self.results.append(result)
    
    def save_to_csv(self, mode='append', auto_save=False):
        """
        LÆ°u káº¿t quáº£ vÃ o CSV
        
        Args:
            mode: 'append' Ä‘á»ƒ thÃªm vÃ o file cÅ©, 'overwrite' Ä‘á»ƒ ghi Ä‘Ã¨
            auto_save: Náº¿u True, chá»‰ lÆ°u káº¿t quáº£ má»›i nháº¥t vÃ  xÃ³a khá»i memory
        """
        if not self.results:
            return
            
        if auto_save:
            # Chá»‰ lÆ°u káº¿t quáº£ cuá»‘i cÃ¹ng
            df_new = pd.DataFrame([self.results[-1]])
        else:
            df_new = pd.DataFrame(self.results)
        
        if mode == 'append' and os.path.exists(self.csv_path):
            # Äá»c file cÅ© vÃ  append
            df_old = pd.read_csv(self.csv_path)
            df_combined = pd.concat([df_old, df_new], ignore_index=True)
            df_combined.to_csv(self.csv_path, index=False)
            if not auto_save:
                print(f"âœ… ÄÃ£ thÃªm {len(df_new)} káº¿t quáº£ vÃ o {self.csv_path}")
        else:
            # Táº¡o file má»›i
            df_new.to_csv(self.csv_path, index=False)
            if not auto_save:
                print(f"âœ… ÄÃ£ táº¡o file má»›i {self.csv_path} vá»›i {len(df_new)} káº¿t quáº£")
        
        # Náº¿u auto_save, xÃ³a káº¿t quáº£ Ä‘Ã£ lÆ°u khá»i memory
        if auto_save and len(self.results) > 0:
            self.results.pop()
    
    def get_summary(self):
        """Láº¥y tÃ³m táº¯t káº¿t quáº£"""
        if not self.results:
            return "ChÆ°a cÃ³ káº¿t quáº£ nÃ o"
        
        df = pd.DataFrame(self.results)
        summary = {
            'total_samples': len(df),
            'avg_precision': df['precision'].mean(),
            'avg_recall': df['recall'].mean(),
            'std_precision': df['precision'].std(),
            'std_recall': df['recall'].std(),
            'min_precision': df['precision'].min(),
            'max_precision': df['precision'].max(),
            'min_recall': df['recall'].min(),
            'max_recall': df['recall'].max()
        }
        return summary
    
    def save_summary_to_txt(self, txt_path='rag_metrics_summary.txt', mode='append'):
        """
        LÆ°u tÃ³m táº¯t káº¿t quáº£ vÃ o file txt
        
        Args:
            txt_path: ÄÆ°á»ng dáº«n file txt
            mode: 'append' Ä‘á»ƒ thÃªm vÃ o file cÅ©, 'overwrite' Ä‘á»ƒ ghi Ä‘Ã¨
        """
        summary = self.get_summary()
        
        if isinstance(summary, str):
            print(summary)
            return
        
        # Táº¡o ná»™i dung
        content = []
        content.append("=" * 60)
        content.append(f"ğŸ“Š RAG METRICS SUMMARY - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        content.append("=" * 60)
        content.append("")
        
        # ThÃ´ng tin chung
        content.append("ğŸ“ˆ GENERAL INFO:")
        content.append(f"   Total Samples: {summary['total_samples']}")
        content.append("")
        
        # Precision metrics
        content.append("ğŸ¯ PRECISION METRICS:")
        content.append(f"   Average:  {summary['avg_precision']:.4f}")
        content.append(f"   Std Dev:  {summary['std_precision']:.4f}")
        content.append(f"   Min:      {summary['min_precision']:.4f}")
        content.append(f"   Max:      {summary['max_precision']:.4f}")
        content.append("")
        
        # Recall metrics
        content.append("ğŸ” RECALL METRICS:")
        content.append(f"   Average:  {summary['avg_recall']:.4f}")
        content.append(f"   Std Dev:  {summary['std_recall']:.4f}")
        content.append(f"   Min:      {summary['min_recall']:.4f}")
        content.append(f"   Max:      {summary['max_recall']:.4f}")
        content.append("")
        
        # ThÃªm thÃ´ng tin vá» retriever náº¿u cÃ³
        if self.results:
            df = pd.DataFrame(self.results)
            if 'retriever_type' in df.columns:
                content.append("âš™ï¸  CONFIGURATION:")
                content.append(f"   Retriever Type: {df['retriever_type'].iloc[0]}")
                if 'embedding_model' in df.columns:
                    content.append(f"   Embedding Model: {df['embedding_model'].iloc[0]}")
                if 'limit' in df.columns:
                    content.append(f"   Retrieval Limit: {df['limit'].iloc[0]}")
                content.append("")
        
        content.append("=" * 60)
        content.append("")
        
        # Ghi vÃ o file
        write_mode = 'a' if mode == 'append' else 'w'
        with open(txt_path, write_mode, encoding='utf-8') as f:
            f.write('\n'.join(content))
        
        print(f"âœ… ÄÃ£ lÆ°u summary vÃ o {txt_path}")
        
        return summary
    
    def clear(self):
        """XÃ³a káº¿t quáº£ táº¡m trong memory"""
        self.results = []


if __name__ == "__main__":
    import sys
    import os
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from retriever import MMRRetriever, TopKRetriever, KneedleRetriever
    from datasets import load_from_disk
    import numpy as np
    from config import VECTOR_SIZE, QDRANT_API_KEY, QDRANT_URL

    # Khá»Ÿi táº¡o retriever
    retriever = TopKRetriever(
        type='qdrant',               
        embeddingName="BAAI/bge-m3",
        vector_size=VECTOR_SIZE,
        qdrant_api=QDRANT_API_KEY,
        qdrant_url=QDRANT_URL
    )
    
    # Khá»Ÿi táº¡o metrics vÃ  logger
    metrics = RAGMetrics()
    logger = MetricsLogger(csv_path='rag_metrics_results.csv')
    
    # Load data
    subset_path = "data/ViHealthQA_test_200"

    if os.path.exists(subset_path):
        print("ğŸ“ Loading 200-sample subset from disk...")
        samples = load_from_disk(subset_path)
    else:
        print("ğŸ†• Creating 200-sample subset...")
        data = load_from_disk('data/ViHealthQA_test')
        samples = data.select(range(200))
        samples.save_to_disk(subset_path)
        print("ğŸ’¾ Saved subset to disk!")

    print("â¡ï¸ Number of samples:", len(samples))
    avg_precision = []
    avg_recall = []
    
    print("ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡...")
    print("="*50)
    
    for i, sample in enumerate(samples, 1):
        try:
            idx = sample['id']
            question = sample['question']
            answer = sample['answer']

            # Retrieve contexts
            candidates = retriever.vector_search(user_query=question, limit=5)
            retrived_texts = [c.content for c in candidates]
            retrieved_ids = [c.id for c in candidates]

            # TÃ­nh metrics
            precision = metrics.calculate_context_precision(
                question=question, 
                ground_truth=answer, 
                contexts=retrived_texts
            )
            recall = metrics.calculate_context_recall(
                ground_truth=answer, 
                contexts=retrived_texts
            )
            
            avg_precision.append(precision)
            avg_recall.append(recall)
            
            # LÆ°u káº¿t quáº£ vÃ o logger
            logger.add_result(
                sample_id=idx,
                question=question,
                ground_truth=answer,
                precision=precision,
                recall=recall,
                num_contexts=len(retrived_texts),
                retrieved_doc_ids=retrieved_ids,
                retriever_type='TopKRetriever',
                metadata={
                    'embedding_model': 'BAAI/bge-m3',
                    'limit': 5
                }
            )
            
            # ğŸ’¾ LÆ¯U NGAY SAU Má»–I SAMPLE
            logger.save_to_csv(mode='append', auto_save=True)

            # In káº¿t quáº£
            print(f'\nğŸ“ Sample {i}/{len(samples)} - ID: {idx}')
            print(f'â“ Question: {question[:80]}...')
            print(f'âœ… Truth: {answer[:80]}...')
            print(f'ğŸ“„ Retrieved IDs: {", ".join(map(str, retrieved_ids))}')
            print(f'ğŸ“Š Precision: {precision:.3f}')
            print(f'ğŸ“Š Recall: {recall:.3f}')
            print(f'ğŸ’¾ ÄÃ£ lÆ°u vÃ o CSV')
            print('-'*50)
            
        except Exception as e:
            print(f'\nâŒ Lá»—i khi xá»­ lÃ½ sample {i}: {e}')
            print(f'   Bá» qua vÃ  tiáº¿p tá»¥c...')
            print('-'*50)
            continue
    
    # TÃ­nh trung bÃ¬nh
    if avg_precision:
        avg_precision = np.mean(avg_precision)
        avg_recall = np.mean(avg_recall)
        
        print("\n" + "="*50)
        print("ğŸ“ˆ Káº¾T QUáº¢ Tá»”NG Há»¢P:")
        print(f"Average Precision: {avg_precision:.3f}")
        print(f"Average Recall: {avg_recall:.3f}")
        print("="*50)
        
        # Äá»c láº¡i toÃ n bá»™ káº¿t quáº£ tá»« CSV Ä‘á»ƒ tÃ­nh summary
        if os.path.exists(logger.csv_path):
            df_all = pd.read_csv(logger.csv_path)
            logger.results = df_all.to_dict('records')
            
            # LÆ°u summary vÃ o txt
            logger.save_summary_to_txt(txt_path='rag_metrics_summary.txt', mode='append')
            
            # In summary
            summary = logger.get_summary()
            print("\nğŸ“Š SUMMARY:")
            for key, value in summary.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.3f}")
                else:
                    print(f"  {key}: {value}")
    else:
        print("\nâš ï¸ KhÃ´ng cÃ³ káº¿t quáº£ nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng")